{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traffic_SBULSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84fU3LHp4EKk"
      },
      "source": [
        "#Access Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XmagAR3T4IrW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsjOXoUT4J6W"
      },
      "source": [
        "#Authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hS5VfFr4Mdl",
        "outputId": "1564c7d9-1b87-4ba1-c7c2-b612c0a68fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/traffic_improvement/dataset_traffic\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka4pCcJrF7fR",
        "outputId": "6c37fd1c-9b52-4882-908e-2f1f256dfc23"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speed_matrix_2015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "path = \"/content/drive/MyDrive/traffic_improvement/dataset_traffic/speed_matrix_2015\"\n",
        "#df=pd.read_csv('/content/drive/MyDrive/Tweet_Sentiment_Analysis_database/Bitcoin_tweets.csv')"
      ],
      "metadata": {
        "id": "rJlXH-L6VZfj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, Input\n",
        "from keras.layers import LSTM, SimpleRNN, GRU, merge, Masking\n",
        "from keras.models import Model\n",
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import RandomState\n",
        "from random import shuffle\n",
        "import datetime\n",
        "\n",
        "np.random.seed(1024)\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "\n",
        "\n",
        "def train_Bi_LSTM(X, Y, epochs = 30, validation_split = 0.2, patience=20):\n",
        "    speed_input = Input(shape = (X.shape[1], X.shape[2]), name = 'speed')\n",
        "    \n",
        "    main_output = Bidirectional(LSTM(input_shape = (X.shape[1], X.shape[2]), output_dim = X.shape[2], return_sequences=False), merge_mode='ave')(speed_input)\n",
        "    \n",
        "    final_model = Model(input = [speed_input], output = [main_output])\n",
        "    \n",
        "    final_model.summary()\n",
        "    \n",
        "    final_model.compile(loss='mse', optimizer='rmsprop')\n",
        "    \n",
        "    history = LossHistory()\n",
        "    earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=patience, verbose=0, mode='auto')\n",
        "    final_model.fit([X], Y, validation_split = 0.2, nb_epoch = epochs, callbacks=[history, earlyStopping])\n",
        "    \n",
        "    return final_model, history\n",
        "\n",
        "def train_2_Bi_LSTM_mask(X, Y, epochs = 30, validation_split = 0.2, patience=20):\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Masking(mask_value=0.,input_shape=(X.shape[1], X.shape[2])))\n",
        "    model.add(LSTM(100,input_dim = X.shape[2], return_sequences=True, input_shape = (X.shape[1], X.shape[2])))\n",
        "    model.add(LSTM(100,input_dim = X.shape[2], return_sequences=False, input_shape = (X.shape[1], X.shape[2])))\n",
        "\n",
        "    model.add(Dense(X.shape[2]))\n",
        "    model.compile(loss='mse', optimizer='rmsprop')\n",
        "\n",
        "    history = LossHistory()\n",
        "    earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=patience, verbose=0, mode='auto')\n",
        "    model.fit(X, Y, validation_split = 0.2, epochs = epochs, callbacks=[history, earlyStopping])\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def train_2_Bi_LSTM(X, Y, epochs = 30, validation_split = 0.2, patience=20):\n",
        "    speed_input = Input(shape = (X.shape[1], X.shape[2]), name = 'speed')\n",
        "    \n",
        "    lstm_output = Bidirectional(LSTM(input_shape = (X.shape[1], X.shape[2]), output_dim = X.shape[2], return_sequences=True), merge_mode='ave')(speed_input)\n",
        "    \n",
        "    main_output = LSTM(input_shape = (X.shape[1], X.shape[2]), output_dim = X.shape[2])(lstm_output)\n",
        "    \n",
        "    final_model = Model(input = [speed_input], output = [main_output])\n",
        "    \n",
        "    final_model.summary()\n",
        "    \n",
        "    final_model.compile(loss='mse', optimizer='rmsprop')\n",
        "    \n",
        "    history = LossHistory()\n",
        "    earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=patience, verbose=0, mode='auto')\n",
        "    final_model.fit([X], Y, validation_split = 0.2, epochs = epochs, callbacks=[history, earlyStopping])\n",
        "    \n",
        "    return final_model, history"
      ],
      "metadata": {
        "id": "O_pK1O3MjQ_d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqEymW4DMTp2",
        "outputId": "4b108bd9-ef5c-4107-cd8b-14674d35a6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "speedMatrix shape: (105120, 323)\n",
            "time lag : 10\n",
            "X_full shape:  (105110, 10, 323) Y_full shape: (105110, 323)\n",
            "train : valid : test =  0.9 0.0 0.1\n",
            "X_train shape:  (94599, 10, 323) Y_train shape: (94599, 323)\n",
            "X_valid shape:  (0, 10, 323) Y_valid shape: (0, 323)\n",
            "X_test shape:  (10511, 10, 323) Y_test shape: (10511, 323)\n",
            "X_full max: 100.0\n",
            "#######################################################\n",
            "model_2_Bi_LSTM\n",
            "time_lag 10\n",
            "Epoch 1/30\n",
            "2365/2365 [==============================] - 110s 41ms/step - loss: 0.0060 - val_loss: 0.0037\n",
            "Epoch 2/30\n",
            "2365/2365 [==============================] - 90s 38ms/step - loss: 0.0035 - val_loss: 0.0031\n",
            "Epoch 3/30\n",
            "2365/2365 [==============================] - 89s 38ms/step - loss: 0.0030 - val_loss: 0.0030\n",
            "Epoch 4/30\n",
            "2365/2365 [==============================] - 95s 40ms/step - loss: 0.0027 - val_loss: 0.0027\n",
            "Epoch 5/30\n",
            "2365/2365 [==============================] - 112s 47ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 6/30\n",
            "2365/2365 [==============================] - 109s 46ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 7/30\n",
            "2365/2365 [==============================] - 90s 38ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 8/30\n",
            "2365/2365 [==============================] - 90s 38ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 9/30\n",
            "2365/2365 [==============================] - 95s 40ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 10/30\n",
            "2365/2365 [==============================] - 91s 38ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 11/30\n",
            "2365/2365 [==============================] - 87s 37ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 12/30\n",
            "2365/2365 [==============================] - 88s 37ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 13/30\n",
            "2365/2365 [==============================] - 86s 36ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 14/30\n",
            "2365/2365 [==============================] - 88s 37ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 15/30\n",
            "2365/2365 [==============================] - 88s 37ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 16/30\n",
            "2365/2365 [==============================] - 87s 37ms/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 17/30\n",
            "2365/2365 [==============================] - 88s 37ms/step - loss: 0.0019 - val_loss: 0.0020\n",
            "Epoch 18/30\n",
            "2365/2365 [==============================] - 88s 37ms/step - loss: 0.0019 - val_loss: 0.0021\n",
            "Epoch 19/30\n",
            "2365/2365 [==============================] - 88s 37ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 20/30\n",
            "2365/2365 [==============================] - 86s 36ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 21/30\n",
            "2365/2365 [==============================] - 86s 36ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 22/30\n",
            "2365/2365 [==============================] - 88s 37ms/step - loss: 0.0019 - val_loss: 0.0019\n",
            "Epoch 23/30\n",
            "2365/2365 [==============================] - 85s 36ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 24/30\n",
            "2365/2365 [==============================] - 98s 41ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 25/30\n",
            "2365/2365 [==============================] - 92s 39ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 26/30\n",
            "2365/2365 [==============================] - 88s 37ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 27/30\n",
            "2365/2365 [==============================] - 91s 38ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 28/30\n",
            "2365/2365 [==============================] - 90s 38ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 29/30\n",
            "2365/2365 [==============================] - 86s 37ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 30/30\n",
            "2365/2365 [==============================] - 83s 35ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "MAE : 3.009 MAPE : 7.373 STD of MAE: 0.769\n",
            "Epoch :  30\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def Get_Data_Label_Aux_Set(speedMatrix, steps):\n",
        "    cabinets = speedMatrix.columns.values\n",
        "    stamps = speedMatrix.index.values\n",
        "    x_dim = len(cabinets)\n",
        "    time_dim = len(stamps)\n",
        "    \n",
        "    speedMatrix = speedMatrix.iloc[:,:].values\n",
        "    \n",
        "    data_set = []\n",
        "    label_set = []\n",
        "    hour_set = []\n",
        "    dayofweek_set = []\n",
        "\n",
        "    for i in range(time_dim - steps ):\n",
        "        data_set.append(speedMatrix[i : i + steps])\n",
        "        label_set.append(speedMatrix[i + steps])\n",
        "        stamp = stamps[i + steps]\n",
        "        hour_set.append(float(stamp[11:13]))\n",
        "        dayofweek = datetime.datetime.strptime(stamp[0:10], '%Y-%M-%d').strftime('%w')\n",
        "        dayofweek_set.append(float(dayofweek))\n",
        "\n",
        "    data_set = np.array(data_set)\n",
        "    label_set = np.array(label_set)\n",
        "    hour_set = np.array(hour_set)\n",
        "    dayofweek_set = np.array(dayofweek_set)\n",
        "    return data_set, label_set, hour_set, dayofweek_set\n",
        "\n",
        "def SplitData(X_full, Y_full, hour_full, dayofweek_full, train_prop = 0.7, valid_prop = 0.2, test_prop = 0.1):\n",
        "    n = Y_full.shape[0]\n",
        "    indices = np.arange(n)\n",
        "    RS = RandomState(1024)\n",
        "    RS.shuffle(indices)\n",
        "    sep_1 = int(float(n) * train_prop)\n",
        "    sep_2 = int(float(n) * (train_prop + valid_prop))\n",
        "    print ('train : valid : test = ', train_prop, valid_prop, test_prop)\n",
        "    train_indices = indices[:sep_1]\n",
        "    valid_indices = indices[sep_1:sep_2]\n",
        "    test_indices = indices[sep_2:]\n",
        "    X_train = X_full[train_indices]\n",
        "    X_valid = X_full[valid_indices]\n",
        "    X_test = X_full[test_indices]\n",
        "    Y_train = Y_full[train_indices]\n",
        "    Y_valid = Y_full[valid_indices]\n",
        "    Y_test = Y_full[test_indices]\n",
        "    hour_train = hour_full[train_indices]\n",
        "    hour_valid = hour_full[valid_indices]\n",
        "    hour_test = hour_full[test_indices]\n",
        "    dayofweek_train = dayofweek_full[train_indices]\n",
        "    dayofweek_valid = dayofweek_full[valid_indices]\n",
        "    dayofweek_test = dayofweek_full[test_indices]\n",
        "    return X_train, X_valid, X_test, \\\n",
        "            Y_train, Y_valid, Y_test, \\\n",
        "            hour_train, hour_valid, hour_test, \\\n",
        "            dayofweek_train, dayofweek_valid, dayofweek_test\n",
        "            \n",
        "def MeasurePerformance(Y_test_scale, Y_pred, X_max, model_name = 'default', epochs = 30, model_time_lag = 10):\n",
        "\n",
        "    time_num = Y_test_scale.shape[0]\n",
        "    loop_num = Y_test_scale.shape[1]\n",
        "\n",
        "    difference_sum = np.zeros(time_num)\n",
        "    diff_frac_sum = np.zeros(time_num)\n",
        "\n",
        "    for loop_idx in range(loop_num):\n",
        "        true_speed = Y_test_scale[:,loop_idx] * X_max\n",
        "        predicted_speed = Y_pred[:,loop_idx] * X_max\n",
        "        diff = np.abs( true_speed - predicted_speed )\n",
        "        diff_frac = diff / true_speed\n",
        "        difference_sum += diff\n",
        "        diff_frac_sum += diff_frac\n",
        "        \n",
        "    difference_avg = difference_sum / loop_num\n",
        "    MAPE = diff_frac_sum / loop_num * 100\n",
        "    \n",
        "    print('MAE :', round(np.mean(difference_avg),3), 'MAPE :', round(np.mean(MAPE),3), 'STD of MAE:', round(np.std(difference_avg),3))\n",
        "    print('Epoch : ' , epochs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    #######################################################\n",
        "    # load 2015 speed data\n",
        "    #######################################################\n",
        "    #speedMatrix = pd.read_pickle('speed_matrix_2015')\n",
        "    speedMatrix = pd.read_pickle(path)\n",
        "    print('speedMatrix shape:', speedMatrix.shape)\n",
        "    loopgroups_full = speedMatrix.columns.values\n",
        "    \n",
        "    time_lag = 10\n",
        "    print('time lag :', time_lag)\n",
        "    \n",
        "    X_full, Y_full, hour_full, dayofweek_full = Get_Data_Label_Aux_Set(speedMatrix, time_lag)\n",
        "    print('X_full shape: ', X_full.shape, 'Y_full shape:', Y_full.shape)\n",
        "    \n",
        "    #######################################################\n",
        "    # split full dataset into training, validation and test dataset\n",
        "    #######################################################\n",
        "    X_train, X_valid, X_test, \\\n",
        "        Y_train, Y_valid, Y_test, \\\n",
        "        hour_train, hour_valid, hour_test, \\\n",
        "        dayofweek_train, dayofweek_valid, dayofweek_test \\\n",
        "                    = SplitData(X_full, Y_full, hour_full, dayofweek_full, train_prop = 0.9, valid_prop = 0.0, test_prop = 0.1)\n",
        "    print('X_train shape: ', X_train.shape, 'Y_train shape:', Y_train.shape)\n",
        "    print('X_valid shape: ', X_valid.shape, 'Y_valid shape:', Y_valid.shape)\n",
        "    print('X_test shape: ' , X_test.shape,  'Y_test shape:',  Y_test.shape)\n",
        "    \n",
        "    #######################################################\n",
        "    # bound training data to 0 to 100\n",
        "    # get the max value of X to scale X\n",
        "    #######################################################\n",
        "    X_train = np.clip(X_train, 0, 100)\n",
        "    X_test = np.clip(X_test, 0, 100)\n",
        "\n",
        "    X_max = np.max([np.max(X_train), np.max(X_test)])\n",
        "    X_min = np.min([np.min(X_train), np.min(X_test)])\n",
        "    print('X_full max:', X_max)\n",
        "    \n",
        "    #######################################################\n",
        "    # scale data into 0~1\n",
        "    #######################################################\n",
        "    X_train_scale = X_train / X_max\n",
        "    X_test_scale = X_test / X_max\n",
        "    \n",
        "    Y_train_scale = Y_train / X_max\n",
        "    Y_test_scale = Y_test / X_max\n",
        "    \n",
        "    #model_epoch = 100\n",
        "    #patience = 20\n",
        "    \n",
        "    print(\"#######################################################\")\n",
        "    print(\"model_2_Bi_LSTM\")\n",
        "    print(\"time_lag\", time_lag)\n",
        "    #model_2_Bi_LSTM, history_2_Bi_LSTM = train_2_Bi_LSTM_mask(X_train_scale, Y_train_scale, epochs = model_epoch,  validation_split = 0.2, patience=20)\n",
        "    model_2_Bi_LSTM, history_2_Bi_LSTM = train_2_Bi_LSTM_mask(X_train_scale, Y_train_scale)\n",
        "    model_2_Bi_LSTM.save('Model_2_Bi_LSTM_' + str(len(history_2_Bi_LSTM.losses))+ 'ep' + '_tl' + str(time_lag)+ '.h5')\n",
        "    Y_pred_test = model_2_Bi_LSTM.predict(X_test_scale)\n",
        "    MeasurePerformance(Y_test_scale, Y_pred_test, X_max, model_name = 'default', epochs = len(history_2_Bi_LSTM.losses), model_time_lag = 10)"
      ]
    }
  ]
}